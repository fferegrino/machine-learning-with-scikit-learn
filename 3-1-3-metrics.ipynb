{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30359ece",
   "metadata": {},
   "source": [
    "# Métricas\n",
    "\n",
    "Como parte de todas las herramientas que nos ofrece scikit-learn, también nos ofrece una forma de medir qué tan buenas son las predicciones hechas por los modelos.\n",
    "\n",
    "Las métricas juegan un papel crucial en la evaluación y selección de modelos de aprendizaje automático. Estas métricas nos permiten cuantificar el rendimiento de nuestros modelos en términos de precisión, robustez y generalización. Y a la vez, también nos permiten comunicar el desempeño de los modelos con otras personas, ajenas a nuestro equipo de machine learning.\n",
    "\n",
    "scikit-learn ofrece más de 30 métricas, son muchas para cubrirlas en un solo video, pero los conceptos y código que verás en esta lección se pueden aplicar a todas ellas puesto que siguen una interfaz muy similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f768077a",
   "metadata": {},
   "source": [
    "## Métricas de clasificación\n",
    "\n",
    "Primero, vamos a crear un pequeño dataset que usaremos para probar las métricas de clasificación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea0043c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_pred = np.array(\n",
    "    [0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0]\n",
    ")\n",
    "\n",
    "y_true = np.array(\n",
    "    [1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a61676e",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "La exactitud (accuracy) es una medida que representa la proporción de predicciones correctas sobre el total de predicciones realizadas. Es una métrica fácil de entender, pero puede ser engañosa en situaciones donde los datos están desbalanceados.\n",
    "\n",
    "La fórmula es la siguiente:\n",
    "\n",
    "\n",
    "\n",
    "Lo cual quiere decir que va a contar el número de coincidencias entre unos y ceros y dividirlos entre la cantidad total.\n",
    "\n",
    "Para usarla en scikit-learn hay que importarla de <code>sklearn.metrics</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2e94d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f1eb04",
   "metadata": {},
   "source": [
    "Entre más cercano esté el resultado a uno es mejor. Aunque recuerda que en ciertos problemas la exactitud no es la mejor métrica para calificar el desempeño de tu modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9707ee8b",
   "metadata": {},
   "source": [
    "### Precisión\n",
    "\n",
    "La precisión, también conocida como valor predictivo positivo, se utiliza para medir la capacidad del modelo en predecir los casos positivos. Formalmente, se define como el número de verdaderos positivos dividido por la suma de verdaderos positivos y falsos positivos. Aquí un valor cercano a 1 es el ideal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f6fcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9585c853",
   "metadata": {},
   "source": [
    "La métrica de precisión es especialmente útil en situaciones en las que el costo de un falso positivo es alto. Es decir, cuando es más importante evitar los falsos positivos que los falsos negativos. Por ejemplo, piensa en un sistema que se encarga de detectar mensajes de spam, ahí la precisión es la medida más adecuada importante puesto que es importante minimizar el número de falsos positivos, en este caso correos legítimos identificadas como correo basura. Puesto que de no hacerlo podríamos causar que nuestros usuarios pierdan información importante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b59480a",
   "metadata": {},
   "source": [
    "### Recall\n",
    "\n",
    "La métrica de <i>recall</i>, también conocida como sensibilidad o tasa de verdaderos positivos, se utiliza para medir la capacidad del modelo para identificar todos los casos positivos. Formalmente, se define como el número de verdaderos positivos dividido por la suma de verdaderos positivos y falsos negativos. Aquí un valor cercano a 1 es el ideal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c76a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "recall_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8cc370",
   "metadata": {},
   "source": [
    "A diferencia de la precisión, la métrica de recall es especialmente útil en situaciones en las que el costo de un falso negativo es alto. Es decir, cuando es más importante evitar los falsos negativos que los falsos positivos. Piensa en la detección temprana del cáncer, es importante minimizar el número de falsos negativos, pacientes con cáncer que no son identificados, ya que esto puede retrasar el tratamiento y poner en riesgo la vida de la persona. En este caso, el recall es más importante que la precisión."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece8119a",
   "metadata": {},
   "source": [
    "### F1 score\n",
    "\n",
    "La F1 score, también conocida como puntaje F1, es una métrica comúnmente utilizada para evaluar la calidad de un modelo de clasificación. Es una medida única que combina las medidas de precisión y recall en una número único.\n",
    "\n",
    "El F1 score es la media armónica de la precisión y el recall, sus valores oscilan entre 0 y 1, siendo 1 el mejor resultado posible.\n",
    "\n",
    "La fórmula que la representa es la siguiente:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fb5c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e19b7bb",
   "metadata": {},
   "source": [
    "Al ser F1 una combinación entre las dos métricas puede usarse en varios escenarios para medir el desempeño general del modelo. Ideal para cuando se debe encontrar un balance entre ser preciso y recuperar todos los casos posibles. A final de cuentas, la métrica a elegir depende totalmente del problema que estás tratando de resolver."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd19692",
   "metadata": {},
   "source": [
    "## Métricas de regresión\n",
    "\n",
    "Primero, vamos a crear un dataset que podamos usar para demostrar las funciones para calcular las métricas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb44dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([3.4, 5.1, 7.9, 9.2, 11.5, 12.4, 14.3, 17.3])\n",
    "y_pred = np.array([3.5, 5.4, 7.2, 9.1, 11.0, 12.9, 14.8, 16.7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecb3d2a",
   "metadata": {},
   "source": [
    "### Error medio absoluto\n",
    "\n",
    "1. Error medio absoluto o mean absolute error, mide la magnitud promedio de los errores en las predicciones. Se calcula como la media de las diferencias absolutas entre las predicciones y los valores reales.\n",
    "\n",
    "\n",
    "\n",
    "Donde $y_i$  es el valor real de la variable objetivo para la $i$-ésima observación, $\\hat{y_i}$ es la predicción del modelo para la $i$-ésima observación, y $n$ es el número total de observaciones. En este caso, el mejor valor es el que esté cercano a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f34ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b873e78f",
   "metadata": {},
   "source": [
    "Esta es una métrica a considerar cuando se quiere conocer y minimizar la magnitud de el error en las predicciones del modelo. Esta es una métrica fácil de interpretar, puesto que nos permite decir con confianza de qué tamaño es el error, y trata a todos los errores por igual, existe una relación lineal entre el tamaño del error, y el valor de MAE. Usando esta métrica podemos comunicar que nuestro modelo “tiene un error de 10 dólares en promedio” si estás hablando de un modelo que predice precios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5fc306",
   "metadata": {},
   "source": [
    "### Error cuadrático medio\n",
    "\n",
    "El error cuadrático medio, o mean squared error, mide el promedio de los errores al cuadrado en las predicciones. Se calcula como la media de las diferencias al cuadrado entre las predicciones y los valores reales.\n",
    "\n",
    "\n",
    "\n",
    "Donde $y_i$ es el valor real de la variable objetivo para la $i$-ésima observación, $\\hat{y_i}$ es la predicción del modelo para la $i$-ésima observación, y $n$ es el número total de observaciones. Nuevamente, el valor ideal es uno cercano a cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57759590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6959e3d6",
   "metadata": {},
   "source": [
    "Esta métrica no es tan sencilla de interpretar, puesto que no existe una relación lineal entre error en las predicciones y el valor de la métrica. Esta relación es rota al elevar al cuadrado el error. El elevar al cuadrado también tiene el efecto de magnificar los errores grandes. Es decir, cuanto mayor sea la diferencia entre los valores predichos y esperados, mayor será el error cuadrático resultante.  No es una métrica que puedas comunicar tan fácilmente, y es comúnmente empleada de forma interna."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02973dfd",
   "metadata": {},
   "source": [
    "### Raíz del error cuadrático medio\n",
    "\n",
    "La raíz del error cuadrático medio o root mean squared error, RMSE: mide el error cuadrático medio, pero toma la raíz cuadrada para expresar el error en las mismas unidades que la variable objetivo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82ea2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c193501",
   "metadata": {},
   "source": [
    "Esta métrica se utiliza para hacer in poco más interpetable y comunicable el error cuadrático medio, puesto que sacar la raíz cuadrada tiene el efecto de convertir nuestro valor de vuelta a las unidades del problema, nos permite hablar del error en un lenguaje que personas no técnicas entiendan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeabf660",
   "metadata": {},
   "source": [
    "### R-cuadrado\n",
    "\n",
    "La métrica R-cuadrado o R-squared mide la proporción de la varianza en la variable objetivo que puede ser explicada por el modelo. R-squared varía entre 0 y 1, siendo 1 el mejor resultado posible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cf81ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ab40a3",
   "metadata": {},
   "source": [
    "El objetivo principal de R2 es el de evaluar la capacidad del modelo para explicar la variabilidad en los datos de entrada, esta es también una métrica que es más bien usada para elegir entre dos o más modelos, y no es una que sea fácil de compartir con otras personas que no tengan conocimiento previo de ciencia de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0390dabf",
   "metadata": {},
   "source": [
    "## Métricas de agrupamiento\n",
    "\n",
    "Primero, vamos a generar un pequeño dataset para demostrar las métricas, en este caso lo mejor es visualizarlo. Recuerda que la visualización no siempre será posible, puesto que tus datos pueden llegar a tener más de 3 dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29875e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generar datos aleatorios y predicciones con KMeans\n",
    "X, y_true = make_blobs(n_samples=1000, centers=4, cluster_std=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988a93a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_6 = KMeans(n_clusters=6, random_state=42, n_init='auto').fit_predict(X)\n",
    "y_pred_5 = KMeans(n_clusters=5, random_state=42, n_init='auto').fit_predict(X)\n",
    "y_pred_4 = KMeans(n_clusters=4, random_state=42, n_init='auto').fit_predict(X)\n",
    "y_pred_3 = KMeans(n_clusters=3, random_state=42, n_init='auto').fit_predict(X)\n",
    "y_wrong = np.random.randint(4, size=1000)\n",
    "\n",
    "predichos = [y_pred_3, y_pred_4, y_pred_5, y_pred_6]\n",
    "\n",
    "# Crear los subplots lado a lado\n",
    "fig, axs = plt.subplots(1, 2 + len(predichos), figsize=(25, 5))\n",
    "\n",
    "axs[0].scatter(X[:, 0], X[:, 1], c='k', alpha=0.5)\n",
    "axs[0].set_title('Datos originales')\n",
    "\n",
    "for idx, y_preds in enumerate(predichos, 1):\n",
    "    axs[idx].scatter(X[:, 0], X[:, 1], c=y_preds)\n",
    "    axs[idx].set_title(f'{idx+2} clusters encontrados')\n",
    "axs[-1].scatter(X[:, 0], X[:, 1], c=y_wrong)\n",
    "axs[-1].set_title('Mal clusttering')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862e01a6",
   "metadata": {},
   "source": [
    "### Shilouette score\n",
    "\n",
    "Esta mide qué tan bien se agrupan los datos y qué tan separados están los grupos. Esta métrica toma valores entre -1 y 1, donde 1 indica una agrupación perfecta, 0 indica que los grupos se superponen y -1 indica que los puntos están asignados al grupo equivocado. Obviamente, el resultado que estamos esperando es 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668d8b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "c3 = silhouette_score(X, y_pred_3)\n",
    "c4 = silhouette_score(X, y_pred_4)\n",
    "c5 = silhouette_score(X, y_pred_5)\n",
    "c6 = silhouette_score(X, y_pred_6)\n",
    "wrong = silhouette_score(X, y_wrong)\n",
    "\n",
    "print(f'Silhouette Score for 3: {c3:0.2f}, 4: {c4:0.2f}, 5: {c5:0.2f}, 6: {c6:0.2f} and random: {wrong:0.2f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fe0380",
   "metadata": {},
   "source": [
    "### Calinski-Harabasz Index:\n",
    "\n",
    "Este mide la separación entre los grupos y la dispersión dentro de los grupos. Cuanto mayor sea el valor de esta métrica, mejor será la agrupación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c4e2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "\n",
    "c3 = calinski_harabasz_score(X, y_pred_3)\n",
    "c4 = calinski_harabasz_score(X, y_pred_4)\n",
    "c5 = calinski_harabasz_score(X, y_pred_5)\n",
    "c6 = calinski_harabasz_score(X, y_pred_6)\n",
    "wrong = calinski_harabasz_score(X, y_wrong)\n",
    "\n",
    "print(f'Índice Calinski-Harabasz para 3: {c3:0.2f}, 4: {c4:0.2f}, 5: {c5:0.2f}, 6: {c6:0.2f} and random: {wrong:0.2f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b748f0ae",
   "metadata": {},
   "source": [
    "### Davies-Bouldin Index\n",
    "\n",
    "Este mide la \"compacidad\" de cada cluster y la separación entre los clusters. Cuanto menor sea el valor de esta métrica, mejor será la agrupación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c69769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "\n",
    "c3 = davies_bouldin_score(X, y_pred_3)\n",
    "c4 = davies_bouldin_score(X, y_pred_4)\n",
    "c5 = davies_bouldin_score(X, y_pred_5)\n",
    "c6 = davies_bouldin_score(X, y_pred_6)\n",
    "wrong = davies_bouldin_score(X, y_wrong)\n",
    "\n",
    "print(f'Índice Davies-Bouldin para 3: {c3:0.2f}, 4: {c4:0.2f}, 5: {c5:0.2f}, 6: {c6:0.2f} and random: {wrong:0.2f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96104514",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "\n",
    "Y pues ahí lo tienen, estas fueron algunas de las métricas que nos ofrece scikit-learn, si te das cuenta, las funciones para calcular las métricas del aprendizaje supervisado siguen un patrón de: valores verdaderos como primer argumento y valores predecidos después. Igualmente las funciones para el aprendizaje no-supervisado, en donde tienes que pasarle las variables de entrada así como los clústers asignados.\n",
    "\n",
    "Igualmente es importante destacar que cada métrica tiene su propio propósito y que no existe una métrica universalmente mejor para evaluar un modelo, ya que depende del problema en cuestión y de las necesidades del usuario."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "notion_metadata": {
   "archived": false,
   "cover": null,
   "created_by": {
    "id": "84951847-6e2b-487e-acba-6838f60f1102",
    "object": "user"
   },
   "created_time": "2023-03-14T20:34:00.000Z",
   "icon": null,
   "id": "3faf1109-8fe9-4bc0-bcbb-79bb64de2e0c",
   "last_edited_by": {
    "id": "84951847-6e2b-487e-acba-6838f60f1102",
    "object": "user"
   },
   "last_edited_time": "2023-04-21T17:06:00.000Z",
   "object": "page",
   "parent": {
    "database_id": "97ecfa4e-50d1-4827-8791-2199c709d680",
    "type": "database_id"
   },
   "properties": {
    "Assign": {
     "id": "%5DtZZ",
     "people": [],
     "type": "people"
    },
    "Code name": {
     "id": "PT%5CP",
     "rich_text": [],
     "type": "rich_text"
    },
    "Name": {
     "id": "title",
     "title": [
      {
       "annotations": {
        "bold": false,
        "code": false,
        "color": "default",
        "italic": false,
        "strikethrough": false,
        "underline": false
       },
       "href": null,
       "plain_text": "3.1.3 Metrics",
       "text": {
        "content": "3.1.3 Metrics",
        "link": null
       },
       "type": "text"
      }
     ],
     "type": "title"
    },
    "Order": {
     "id": "k_Cb",
     "number": 4.3,
     "type": "number"
    },
    "Real order": {
     "id": "%7Dn_k",
     "rich_text": [
      {
       "annotations": {
        "bold": false,
        "code": false,
        "color": "default",
        "italic": false,
        "strikethrough": false,
        "underline": false
       },
       "href": null,
       "plain_text": "15",
       "text": {
        "content": "15",
        "link": null
       },
       "type": "text"
      }
     ],
     "type": "rich_text"
    },
    "Status": {
     "id": "s%7D%5Ea",
     "status": {
      "color": "green",
      "id": "b6060d48-45e7-42e9-a8a0-8bd803231f7f",
      "name": "Done"
     },
     "type": "status"
    }
   },
   "url": "https://www.notion.so/3-1-3-Metrics-3faf11098fe94bc0bcbb79bb64de2e0c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
