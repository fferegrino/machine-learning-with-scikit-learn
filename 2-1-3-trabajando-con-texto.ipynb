{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29f65ae8",
   "metadata": {},
   "source": [
    "# Trabajando con texto\n",
    "\n",
    "Seguramente habr√°s escuchado hablar de algo llamado ChatGPT. Pues aqu√≠ tiene sus principios: en el an√°lisis de texto.\n",
    "\n",
    "El an√°lisis de texto es un tema que merece probablemente su propio curso, pero podemos ir poniendo las bases empleando scikit-learn.\n",
    "\n",
    "Lo primero, es que hay que recordar que las palabras escritas no son consumibles directamente por los modelos de aprendizaje autom√°tico, sino que necesitamos convertirlas en una representaci√≥n num√©rica que podamos procesar. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7a63ab",
   "metadata": {},
   "source": [
    "## Bag of words ‚Äì CountVectorizer\n",
    "\n",
    "Una de las formas m√°s comunes de hacer esto es mediante el uso de la t√©cnica de bag of words (BoW) o bolsa de palabras, que convierte un texto en una matriz de frecuencia de palabras.\n",
    "\n",
    "Para llevar a cabo este scikit-learn nos ofrece una utilidad que realiza por nosotros lo siguiente:\n",
    "\n",
    " - Parte el texto en tokens, generalmente palabras completas,\n",
    "\n",
    " - Cuenta las ocurrencias de cada uno de estos tokens,\n",
    "\n",
    " - Asigna valores dentro de un vector de acuerdo a el n√∫mero de ocurrencias de cada token en nuestros datos de entrada.\n",
    "\n",
    "Esto lo hace a trav√©s del vectorizador conocido como <code>CountVectorizer</code>, para verlo en acci√≥n, primero hay que generar un conjunto de datos. Por cierto, a un conjunto de datos se le suele llamar corpus, y a cada uno de sus elementos individuales se le conoce como documento. \n",
    "\n",
    "Entonces vamos a generar un corpus con tres documentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252d966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Scikit-learn nos ayuda a trabajar con texto\",\n",
    "    \"Parte el texto en tokens, generalmente palabras completas\",\n",
    "    \"Cuenta las ocurrencias de cada uno de estos tokens\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4ced0a",
   "metadata": {},
   "source": [
    "Importamos el vectorizador ‚Äì nota que estamos importando de <code>sklearn.feature_extraction.text</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29954dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f263232",
   "metadata": {},
   "source": [
    "Creamos un objeto con valores por default ‚Äì y lo entrenamos con nuestro corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca0bd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e379f35",
   "metadata": {},
   "source": [
    "Si llamamos al m√©todo <code>transform</code> pas√°ndole nuestro corpus, el resultado es el esperado: una matriz dispersa puesto que esa es la mejor representaci√≥n de nuestros datos. Podemos convertirla a un arreglo de NumPy con su m√©todo <code>todense</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4909ba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_corpus = count_vectorizer.transform(corpus)\n",
    "transformed_corpus.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f86ff6",
   "metadata": {},
   "source": [
    "S√≠, es una matriz con un mont√≥n de ceros. Si quieres ver a qu√© palabra corresponde cada columna, puedes acceder a la propiedad calculada <code>vocabulary_</code>, as√≠ podr√°s ver que la palabra ‚Äúayuda‚Äù corresponde a la columna cero y que la palabra ‚Äúuno‚Äù corresponde a la columna n√∫mero 20."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b77068",
   "metadata": {},
   "source": [
    "## Transformaci√≥n inversa\n",
    "\n",
    "El <code>CountVectorizer</code> nos ofrece el m√©todo <code>inverse_transform</code> para que a partir de una matriz de vectores, puedas recuperar los tokens que usaste a la entrada. Ten cuidado porque a pesar de que es una transformaci√≥n inversa, esta no es tan fidedigna, puesto que una de las desventajas de esta familia de vectorizadores es que el orden de las palabras se pierde. \n",
    "\n",
    "Lo podemos probar llamando al m√©todo con la matriz que acabamos de conseguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4bf3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer.inverse_transform(transformed_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495d0e28",
   "metadata": {},
   "source": [
    "## Par√°metros extra\n",
    "\n",
    "El <code>CountVectorizer</code> es una de las primeras clases que tiene una gran cantidad de par√°metros para configurar su comportamiento. Entre las m√°s comunes que he viso son usadas est√°n:\n",
    "\n",
    " - <code>binary</code> que por defecto tiene el valor de <code>False</code>, cuando este valor es verdadero, el <code>CountVectorizer</code> se comporta m√°s bien como un one-hot encoder, nuestra matriz resultante consta de unos y ceros.\n",
    "\n",
    " - <code>max_features</code>, este puede ser un n√∫mero que nos indica la cantidad m√°xima de columnas que queremos dentro de nuestra matriz. En el ejemplo anterior ten√≠amos como resultado una matriz de 21 columnas, pero si hubi√©semos establecido <code>max_features</code> con un valor de 10, tendr√≠amos como resultado una matriz de 10 columnas, en donde esas diez columnas contendr√≠an los 10 tokens m√°s frecuentes.\n",
    "\n",
    " - <code>max_df</code> y <code>min_df</code>, estos par√°metros nos permiten eliminar palabras que est√°n sobrerepresentadas y subrepresentadas en nuestro corpus. Estos valores pueden ser flotantes, pueden ir entre 0 y 1 si queremos usarlo como proporci√≥n, o podemos usarlo como entero si queremos contar las ocurrencias netamente.\n",
    "\n",
    "Por ejemplo, si creamos un vectorizador con los siguientes argumentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5da9c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_count_vectorizer = CountVectorizer(\n",
    "    binary = True,\n",
    "    max_features = 6,\n",
    "    min_df = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a3bdf8",
   "metadata": {},
   "source": [
    "Podr√°s ver que como resultado de transformar nuestro corpus obtenemos una matriz de 6 columnas, rellena con solamente unos y ceros ‚Äì y que el vocabulario est√° formado √∫nicamente de los seis tokens con mayor frecuencia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47f22e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_result = modified_count_vectorizer.fit_transform(corpus)\n",
    "print(modified_count_vectorizer.vocabulary_)\n",
    "new_result.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106f5d24",
   "metadata": {},
   "source": [
    "## Cambiando el tokenizer\n",
    "\n",
    "Hay ocasiones en las que queremos tener m√°s control sobre la forma en la que los documentos de nuestro corpus deben ser fragmentados en tokens. Por ejemplo si estamos lidiando con otro idioma, o estamos trabajando con texto que contiene emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f1f794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Un tokenizador que mantiene solo los emojis\n",
    "def emoji_tokenizer(text):\n",
    "    emojis = re.findall(r'[\\U0001F000-\\U0001F6FF]', text)\n",
    "    return emojis\n",
    "\n",
    "print(emoji_tokenizer(\"I üíö üçï\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd608e4",
   "metadata": {},
   "source": [
    "Usamos el tokenziador pas√°ndolo a <code>CountVectorizer</code> en el argumento <code>tokenizer</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc5292",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_corpus = [\n",
    "    \"I üíö üçï\",\n",
    "    \"This üçï was üëé\",\n",
    "    \"I like either üçï or üçî, but not üå≠\",\n",
    "]\n",
    "\n",
    "emoji_vectorizer = CountVectorizer(tokenizer=emoji_tokenizer)\n",
    "X = emoji_vectorizer.fit_transform(emoji_corpus)\n",
    "\n",
    "# Print the feature names and the count matrix\n",
    "print(emoji_vectorizer.vocabulary_)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b12df38",
   "metadata": {},
   "source": [
    "## Ponderaci√≥n Tf-idf ‚Äì TfidfVectorizer\n",
    "\n",
    "En un corpus de texto grande, algunas palabras estar√°n muy sobrerrepresentadas (como \"el\", \"un\", \"es\") y, por lo tanto, tienen poca informaci√≥n significativa sobre el contenido real del documento ‚Äì si estas palabras existen en todos los documentos, no son tan √∫tiles.\n",
    "\n",
    "Si nosotros tomamos los resultados de un <code>CountVectorizer</code> en esos casos, corremos el riesgo de pasarle informaci√≥n innecesaria a nuestro modelo, ocultando as√≠ palabras menos frecuentes, m√°s raras y m√°s interesantes. Para abordar este problema, podemos utilizar t√©cnicas de extracci√≥n de caracter√≠sticas de texto que ponderan las palabras en funci√≥n de su importancia relativa en el corpus.\n",
    "\n",
    "Una t√©cnica com√∫n utilizada para la extracci√≥n de caracter√≠sticas de texto es la frecuencia de t√©rmino ‚Äì frecuencia inversa de documento o (TF-IDF), que mide la importancia relativa de una palabra en un documento en funci√≥n de la frecuencia de esa palabra en el corpus en su conjunto.\n",
    "\n",
    "Scikit-learn nos ofrece una clase llamada <code>TfidfVectorizer</code> que tiene el mismo comportamiento externo que el <code>CountVectorizer</code>: recibe un conjunto de textos y nos da una matriz correspondiente. Adem√°s de que tiene casi los mismos argumentos. \n",
    "\n",
    " > üìö De tarea se te queda probar los ejemplos que vimos pero utilizando el <code>TfidfVectorizer</code>, dime en los comentarios ¬øqu√© notas de diferencia?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5087babd",
   "metadata": {},
   "source": [
    "## Feature hashing\n",
    "\n",
    "Otra de las formas de convertir texto a representaci√≥n num√©rica es a trav√©s del uso del <i>hashing trick</i>, ¬ørecuerdas que lo vimos hace poco? ‚Äì para este caso, scikit-learn nos tiene otra clase: <code>HashingVectorizer</code>, que comparte muchas caracter√≠sticas con los dos vectorizadores anteriormente vistos. Obviamente, tiene las mismas limitantes que ya conocemos, sin embargo puede ser buena alternativa en algunos escenario ‚Äì ah, recuerda, utilizando <i>feature hashing</i> es imposible regresar a los valores originales.\n",
    "\n",
    " > üìö Otra tarea m√°s: prueba los ejemplos que vimos pero utilizando el <code>HashingVectorizer</code>, dime en los comentarios ¬øqu√© notas de diferencia?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779f9755",
   "metadata": {},
   "source": [
    "## Conclusi√≥n\n",
    "\n",
    "Ya vimos m√∫ltiples maneras de transformar nuestro texto en n√∫meros, <code>CountVectorizer</code> en modo cuenta o en modo binario, <code>TfidfVectorizer</code> y <code>HashingVectorizer</code>. El m√©todo que debas usar depender√° de tu caso de uso, pero puedes seguir estas reglas generales:\n",
    "\n",
    " - <code><b>CountVectorizer</b></code> es √∫til para crear una matriz de recuento de palabras cuando es importante el n√∫mero absoluto de ocurrencias de cada palabra en el texto.\n",
    "\n",
    " - <code><b>TfidfVectorizer</b></code> es √∫til cuando se pondera la importancia de cada palabra en el texto en funci√≥n de la frecuencia con que aparece en el corpus.\n",
    "\n",
    " - <code><b>HashingVectorizer</b></code> es √∫til para trabajar con conjuntos de datos muy grandes que no caben en memoria y para reducir la dimensionalidad del espacio de caracter√≠sticas.\n",
    "\n",
    "Espero ahora te quede m√°s claro cu√°l es el primer paso antes de tratar de alimentar texto en tus modelos de machine learning, recuerda que debes practicar y en los recursos encontrar√°s ejemplos pr√°cticos en los que se usan algunos vectorizadores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "notion_metadata": {
   "archived": false,
   "cover": null,
   "created_by": {
    "id": "84951847-6e2b-487e-acba-6838f60f1102",
    "object": "user"
   },
   "created_time": "2023-03-14T19:58:00.000Z",
   "icon": null,
   "id": "a6e5be72-54f0-4f1b-af9e-552fdf4096f4",
   "last_edited_by": {
    "id": "84951847-6e2b-487e-acba-6838f60f1102",
    "object": "user"
   },
   "last_edited_time": "2023-04-21T17:05:00.000Z",
   "object": "page",
   "parent": {
    "database_id": "97ecfa4e-50d1-4827-8791-2199c709d680",
    "type": "database_id"
   },
   "properties": {
    "Assign": {
     "id": "%5DtZZ",
     "people": [],
     "type": "people"
    },
    "Code name": {
     "id": "PT%5CP",
     "rich_text": [],
     "type": "rich_text"
    },
    "Name": {
     "id": "title",
     "title": [
      {
       "annotations": {
        "bold": false,
        "code": false,
        "color": "default",
        "italic": false,
        "strikethrough": false,
        "underline": false
       },
       "href": null,
       "plain_text": "2.1.3 Trabajando con texto",
       "text": {
        "content": "2.1.3 Trabajando con texto",
        "link": null
       },
       "type": "text"
      }
     ],
     "type": "title"
    },
    "Order": {
     "id": "k_Cb",
     "number": 3.13,
     "type": "number"
    },
    "Real order": {
     "id": "%7Dn_k",
     "rich_text": [
      {
       "annotations": {
        "bold": false,
        "code": false,
        "color": "default",
        "italic": false,
        "strikethrough": false,
        "underline": false
       },
       "href": null,
       "plain_text": "08",
       "text": {
        "content": "08",
        "link": null
       },
       "type": "text"
      }
     ],
     "type": "rich_text"
    },
    "Status": {
     "id": "s%7D%5Ea",
     "status": {
      "color": "green",
      "id": "b6060d48-45e7-42e9-a8a0-8bd803231f7f",
      "name": "Done"
     },
     "type": "status"
    }
   },
   "url": "https://www.notion.so/2-1-3-Trabajando-con-texto-a6e5be7254f04f1baf9e552fdf4096f4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
