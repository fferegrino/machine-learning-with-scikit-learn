{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92da298c",
   "metadata": {},
   "source": [
    "# Regresi√≥n log√≠stica\n",
    "\n",
    "Ya sabes que existe una regresi√≥n que en lugar de predecir valores num√©ricos continuos nos ayuda a hacer clasificaciones. Esa regresi√≥n es la regresi√≥n log√≠stica, y en scikit-learn, la implementaci√≥n de este algoritmo se encuentra en la clase <code>LogisticRegression</code>.\n",
    "\n",
    "La regresi√≥n log√≠stica es un modelo de aprendizaje supervisado que se utiliza com√∫nmente para problemas de clasificaci√≥n.\n",
    "\n",
    "Dado un conjunto de caracter√≠sticas, la regresi√≥n log√≠stica estima la probabilidad de que una instancia pertenezca a una clase en particular. Esta probabilidad se transforma en una etiqueta de clase utilizando un umbral de decisi√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27865d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un dataset\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(n_samples=1000, random_state=42, noise=0.40)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f151bb",
   "metadata": {},
   "source": [
    "Luego, se instancia un objeto de la clase <code>LogisticRegression</code> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e143371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675ce1fb",
   "metadata": {},
   "source": [
    "Y se ajusta a los datos de entrenamiento utilizando el m√©todo <code>fit</code>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3334fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d61aa7",
   "metadata": {},
   "source": [
    "Una vez entrenado, el modelo se puede utilizar para hacer predicciones en los datos de prueba utilizando el m√©todo <code>predict</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516c1846",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409d71ad",
   "metadata": {},
   "source": [
    "La verdad es que no hay mucha ciencia en eso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ac5c23",
   "metadata": {},
   "source": [
    "## Predict proba\n",
    "\n",
    "En problemas de clasificaci√≥n los clasificadores de scikit-learn tienen un m√©todo llamado <code>predict_proba</code> que puedes utilizar para obtener un estimado de qu√© tan probable es que una instancia pertenezca a una clase u otra.\n",
    "\n",
    "Por ejemplo, puedes llamar al m√©todo predict proba sobre nuestro modelo y nuestros datos de entrada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52d73e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = lr.predict_proba(X_test)\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9e7de3",
   "metadata": {},
   "source": [
    "En este caso, como estamos hablando de un problema de clasificaci√≥n binaria, <code>probabilities</code> es una matriz de dos columnas, en donde la primera columna representa la probabilidad de que la muestra pertenezca a la clase negativa y la segunda a la positiva.\n",
    "\n",
    "El predecir las probabilidades en lugar de obtener una clasificaci√≥n dura es √∫til en algunos casos, para conocer m√°s, te invito a que veas los recursos de esta lecci√≥n.\n",
    "\n",
    "Recuerda adem√°s que todos los clasificadores de scikit-learn tienen este m√©todo, y no solo la regresi√≥n lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0414427a",
   "metadata": {},
   "source": [
    "## Argumentos\n",
    "\n",
    "La clase <code>LogisticRegression</code> en scikit-learn tiene una gran cantidad de par√°metros que permiten personalizar el modelo seg√∫n las necesidades espec√≠ficas del problema, algunos de los comunes y con los que te recomiendo que juegues al momento de trabajar\n",
    "\n",
    " - <code>penalty</code>: especifica la norma de regularizaci√≥n a utilizar en el modelo. Las opciones comunes son ‚ÄúL1‚Äù, ‚ÄúL2‚Äù y ‚Äúelasticnet‚Äù. El valor por default es ‚ÄúL2‚Äù. En general, mi recomendaci√≥n es que trates de no usar ‚ÄúL1‚Äù con la regresi√≥n log√≠stica. \n",
    "\n",
    " - <code>tol</code>: especifica la tolerancia para la detecci√≥n de convergencia del algoritmo de optimizaci√≥n. Al ser este un algoritmo iterativo, es importante establecer un valor de tolerancia, en caso de que el algoritmo llegue a un punto en el que los valores no cambien lo suficiente, poder detener el entrenamiento.\n",
    "\n",
    " - <code>max_iter</code>: siguiendo en el tema de las iteraciones, tambi√©n es posible establecer un n√∫mero m√°ximo de estas.\n",
    "\n",
    " - <code>C</code>: es un valor que controla la fuerza con la que la regularizaci√≥n es aplicada. <code>C</code> tiene la peculiaridad de ser un valor que afecta inversamente a la regularizaci√≥n, entre m√°s peque√±o sea este valor, m√°s fuerte ser√° la regularizaci√≥n aplicada.\n",
    "\n",
    " - <code>class_weight</code>: este argumento es √∫til cuando est√°s lidiando con un problema en donde haya un desbalance en los datos.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ae53f8",
   "metadata": {},
   "source": [
    "## Ejemplo de uso de <code>class_weight</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a09f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import classification_report_comparison\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# Create an imbalanced classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5,\n",
    "                           weights=[0.9], random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Fit Logistic Regression with class_weight='balanced'\n",
    "balanced_lr = LogisticRegression(class_weight='balanced')\n",
    "balanced_lr.fit(X_train, y_train)\n",
    "\n",
    "vanilla_lr = LogisticRegression()\n",
    "vanilla_lr.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Make predictions on the testing set\n",
    "balanced_y_pred = balanced_lr.predict(X_test)\n",
    "vanilla_y_pred = vanilla_lr.predict(X_test)\n",
    "\n",
    "classification_report_comparison(y_test, {\"Balanced\": balanced_y_pred, \"No balance\": vanilla_y_pred})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293c88a8",
   "metadata": {},
   "source": [
    " > üìö De tarea, ¬øpor qu√© no intentas jugar un poco m√°s con los par√°metros? utiliza la funci√≥n <code>classification_report_comparison</code>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "notion_metadata": {
   "archived": false,
   "cover": null,
   "created_by": {
    "id": "84951847-6e2b-487e-acba-6838f60f1102",
    "object": "user"
   },
   "created_time": "2023-03-14T20:31:00.000Z",
   "icon": null,
   "id": "64f4d55c-a830-47cb-8825-7baa26afbd1b",
   "last_edited_by": {
    "id": "84951847-6e2b-487e-acba-6838f60f1102",
    "object": "user"
   },
   "last_edited_time": "2023-04-21T17:07:00.000Z",
   "object": "page",
   "parent": {
    "database_id": "97ecfa4e-50d1-4827-8791-2199c709d680",
    "type": "database_id"
   },
   "properties": {
    "Assign": {
     "id": "%5DtZZ",
     "people": [],
     "type": "people"
    },
    "Code name": {
     "id": "PT%5CP",
     "rich_text": [],
     "type": "rich_text"
    },
    "Name": {
     "id": "title",
     "title": [
      {
       "annotations": {
        "bold": false,
        "code": false,
        "color": "default",
        "italic": false,
        "strikethrough": false,
        "underline": false
       },
       "href": null,
       "plain_text": "4.2.1 Regresi√≥n log√≠stica",
       "text": {
        "content": "4.2.1 Regresi√≥n log√≠stica",
        "link": null
       },
       "type": "text"
      }
     ],
     "type": "title"
    },
    "Order": {
     "id": "k_Cb",
     "number": 5.21,
     "type": "number"
    },
    "Real order": {
     "id": "%7Dn_k",
     "rich_text": [
      {
       "annotations": {
        "bold": false,
        "code": false,
        "color": "default",
        "italic": false,
        "strikethrough": false,
        "underline": false
       },
       "href": null,
       "plain_text": "22 - RegresionLogistica",
       "text": {
        "content": "22 - RegresionLogistica",
        "link": null
       },
       "type": "text"
      }
     ],
     "type": "rich_text"
    },
    "Status": {
     "id": "s%7D%5Ea",
     "status": {
      "color": "green",
      "id": "b6060d48-45e7-42e9-a8a0-8bd803231f7f",
      "name": "Done"
     },
     "type": "status"
    }
   },
   "url": "https://www.notion.so/4-2-1-Regresi-n-log-stica-64f4d55ca83047cb88257baa26afbd1b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
