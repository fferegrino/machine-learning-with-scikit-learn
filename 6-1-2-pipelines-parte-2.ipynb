{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b97c14c6",
   "metadata": {},
   "source": [
    "## Pipelines compuestos\n",
    "\n",
    "Hasta ahora hemos visto la utilidad de los <i>pipelines</i> y cómo es que podemos usarlos. Pero hemos creado pipelines bastante sencillos, ¿no crees?\n",
    "\n",
    "Vamos a crear uno un poco más complicado, pero para eso vamos a necesitar un dataset un poco más complicado también:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280890a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_complex_data\n",
    "\n",
    "dataset = load_complex_data()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dfbb00",
   "metadata": {},
   "source": [
    "Son 6 columnas, una de ellas es un <code>ID</code>, <code>job</code>, <code>marital</code> son categorías, <code>balance</code>, <code>age</code> y <code>loyalty</code> son numéricas y <code>subscribed</code>, la variable objetivo es categórica binaria. \n",
    "\n",
    "Vamos a preparar este dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5147e232",
   "metadata": {},
   "source": [
    "## <code>ColumnTransformer</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5637fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot_encode_categories = ColumnTransformer([\n",
    "    (\n",
    "        'one_hot_encode_categories', # Nombre de la transformación\n",
    "        OneHotEncoder(sparse_output=False), # Transformación a aplicar\n",
    "        [\"job\", 'marital'] # Columnas involucradas\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884de36f",
   "metadata": {},
   "source": [
    "Vamos a ver qué es lo que hace con nuestro dataset después de entrenarlo con <code>fit</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7740c6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encode_categories.fit(dataset)\n",
    "\n",
    "transformed_dataset = one_hot_encode_categories.transform(dataset)\n",
    "transformed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fd99c8",
   "metadata": {},
   "source": [
    "Uno puede acceder a los elementos de <code>ColumnTransformer</code> con el atributo <code>named_transformers_</code> y de ahí vamos a acceder al atributo <code>categories_</code> para recuperar los encabezados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17a7986",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = one_hot_encode_categories.named_transformers_['one_hot_encode_categories'].categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c674ba4a",
   "metadata": {},
   "source": [
    "Podemos usar esta función que cree para ver esta matriz como un dataframe con las columnas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5076a384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import show_transformed_data\n",
    "\n",
    "show_transformed_data(transformed_dataset, cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade9f17e",
   "metadata": {},
   "source": [
    "## Nested pipelines\n",
    "\n",
    "Vamos a hacer algo con las variable <code>age</code>. Lo primero a notar es que la variable <code>age</code> tiene valores nulos, hay que imputar sus valores y después vamos a discretizarla, hagamos un pipeline para eso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36a4607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "handle_age_pipeline = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='mean')),\n",
    "    ('discretize', KBinsDiscretizer(encode=\"onehot-dense\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0be2f71",
   "metadata": {},
   "source": [
    "Si lo probamos pasando la columna <code>age</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa77369",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_age_pipeline.fit_transform(dataset[['age']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44da3212",
   "metadata": {},
   "source": [
    "Vamos a envolver este pipeline en un column transformer para que funcione directamente con el dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378996fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_age_pipeline = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='mean')),\n",
    "    ('discretize', KBinsDiscretizer(encode=\"onehot-dense\"))\n",
    "])\n",
    "\n",
    "handle_age_transformer = ColumnTransformer([\n",
    "    (\n",
    "        'handle_age_transformer', # Nombre de la transformación\n",
    "        handle_age_pipeline, # Transformación a aplicar\n",
    "        [\"age\"] # Columnas involucradas\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a506b137",
   "metadata": {},
   "source": [
    "Y podemos verificar que funciona:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed42b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_age_transformer.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65372157",
   "metadata": {},
   "source": [
    "## Dejando variables sin transformar\n",
    "\n",
    "Puedes utilizar la cadena <code>passthrough</code> para dejar variables pasar sin ninguna transformación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f33991",
   "metadata": {},
   "outputs": [],
   "source": [
    "let_loyalty_pass_transformer = ColumnTransformer([\n",
    "    (\n",
    "        'leave_loyalty_alone',\n",
    "        'passthrough',\n",
    "        ['loyalty']\n",
    "    )\n",
    "])\n",
    "\n",
    "let_loyalty_pass_transformer.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d0664a",
   "metadata": {},
   "source": [
    "## <code>FeatureUnion</code> para juntar todo\n",
    "\n",
    "Vamos a re-crear todo lo que acabamos de hacer arriba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7f727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ya lo vimos más arriba\n",
    "one_hot_encode_categories = ColumnTransformer([\n",
    "    (\n",
    "        'one_hot_encode_categories', # Nombre de la transformación\n",
    "        OneHotEncoder(sparse_output=False), # Transformación a aplicar\n",
    "        [\"job\", 'marital'] # Columnas involucradas\n",
    "    )\n",
    "])\n",
    "\n",
    "# Ya lo vimos más arriba\n",
    "handle_age_pipeline = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='mean')),\n",
    "    ('discretize', KBinsDiscretizer(encode=\"onehot-dense\"))\n",
    "])\n",
    "handle_age_transformer = ColumnTransformer([\n",
    "    (\n",
    "        'handle_age_transformer', # Nombre de la transformación\n",
    "        handle_age_pipeline, # Transformación a aplicar\n",
    "        [\"age\"] # Columnas involucradas\n",
    "    )\n",
    "])\n",
    "\n",
    "# Ya lo vimos más arriba\n",
    "let_loyalty_pass_transformer = ColumnTransformer([\n",
    "    (\n",
    "        'leave_loyalty_alone',\n",
    "        'passthrough',\n",
    "        ['loyalty']\n",
    "    )\n",
    "])\n",
    "\n",
    "# Este es nuevo\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scale_balance = ColumnTransformer([\n",
    "    ('scale_balance', StandardScaler(), ['balance'])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b81747b",
   "metadata": {},
   "source": [
    "Recuerda que gracias a <code>ColumnTransformer</code> cada uno de estos transformadores individualmente actúa sobre solamente unas cuantas columnas del dataset y descarta el resto. Pero en realidad lo que queremos es generar un dataset único.\n",
    "\n",
    "Podemos utilizar la clase <code>FeaturUnion</code> para juntar nuestras características horizontalmente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f5f530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "all_the_features = FeatureUnion([\n",
    "    ('one_hot_encode_categories', one_hot_encode_categories),\n",
    "    ('handle_age_transformer', handle_age_transformer),\n",
    "    ('let_loyalty_pass_transformer', let_loyalty_pass_transformer),\n",
    "    ('scale_balance', scale_balance)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be701831",
   "metadata": {},
   "source": [
    "Y si llamamos a <code>fit_transform</code> obtendremos un nuevo dataset ya transformado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62fb7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = all_the_features.fit_transform(dataset)\n",
    "transformed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea619bd",
   "metadata": {},
   "source": [
    "Este daataset tiene 22 columnas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d513c5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1427db",
   "metadata": {},
   "source": [
    " 15 de ellas provienen de las variables categóricas <code>job</code>, <code>marital</code>, 5 proviene de la columna <code>age</code> que binarizamos, y luego <code>balance</code> y <code>loyalty</code> son las dos restantes. Y bueno, en el proceso nos deshicimos de la columna <code>ID</code> que no nos sirve para nada en este caso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed9b721",
   "metadata": {},
   "source": [
    "## Entrenando un modelo\n",
    "\n",
    "Para terminar, vamos a agregar un modelo de machine learning al final para que sea la joya de la corona y tengamos todo en un mismo lugar.\n",
    "\n",
    "Lo primero, vamos a utilizar <code>clone</code> para crear copias sin entrenar de todo nuestro pipeline ya creado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62461b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "feature_transformer = clone(all_the_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a315f91",
   "metadata": {},
   "source": [
    "Creamos el pipeline final:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c5be17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "inference_pipeline = Pipeline([\n",
    "    ('featurize', feature_transformer),\n",
    "    ('classifier', LogisticRegression()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5944cabc",
   "metadata": {},
   "source": [
    "Para visualizar qué es lo que está sucediendo, puedes visualizarlo simplemente dejándolo solo en una celda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ab0534",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208eba5d",
   "metadata": {},
   "source": [
    "Ahora si, vamos a entrenarlo como cualquier otro estimador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95303b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_pipeline.fit(\n",
    "    dataset,\n",
    "    dataset['subscribed']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5cbb88",
   "metadata": {},
   "source": [
    "Y si creamos un nuevo ejemplo, podemos ejecutar predict sin ningún problema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62a68c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "nuevos_datos = pd.DataFrame([\n",
    "    {\n",
    "        \"ID\": 2432,\n",
    "        \"job\": \"technician\",\n",
    "        \"marital\": \"single\",\n",
    "        \"balance\": 90,\n",
    "        \"age\": 34,\n",
    "        \"loyalty\": 0.5\n",
    "    }\n",
    "])\n",
    "\n",
    "nuevos_datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e114a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_pipeline.predict(nuevos_datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c72c36",
   "metadata": {},
   "source": [
    "Y ya está, ¡ahora lo único que debes almacenar y compartir es el objeto <code>inference_pipeline</code>!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55013f21",
   "metadata": {},
   "source": [
    "## ¿Cuándo usarlos y cuando no?\n",
    "\n",
    "Como puedes ver, los pipelines son muy útiles en muchos casos y ofrecen diversas ventajas. Sin embargo, hay situaciones en las que no son la mejor opción. A continuación, se presentan algunos consejos generales sobre cuándo usar o no usar pipelines:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480cc484",
   "metadata": {},
   "source": [
    "### <b>Cuándo usar pipelines:</b>\n",
    "\n",
    " 1. Procesamiento secuencial: Si tu flujo de trabajo de aprendizaje automático sigue una estructura secuencial, los pipelines son ideales para organizar y simplificar el proceso.\n",
    "\n",
    " 1. Validación cruzada y ajuste de hiperparámetros: Los pipelines facilitan la validación cruzada y el ajuste de hiperparámetros, asegurando que las transformaciones de datos se apliquen de manera consistente y evitando problemas como la fuga de datos.\n",
    "\n",
    " 1. Reproducibilidad y mantenibilidad: Si deseas mejorar la reproducibilidad y mantenibilidad de tu código, los pipelines son una excelente opción, ya que permiten encapsular todo el flujo de trabajo en una sola estructura.\n",
    "\n",
    " 1. Colaboración en proyectos: Si estás trabajando en un equipo, los pipelines pueden facilitar la colaboración al proporcionar una representación clara y coherente de las diferentes etapas del proceso de aprendizaje automático."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8de46a",
   "metadata": {},
   "source": [
    "### <b>Cuándo no usar pipelines:</b>\n",
    "\n",
    " 1. Preprocesamiento complejo: Si tu conjunto de datos requiere de operaciones que no pueden representarse fácilmente como transformadores de scikit-learn, es posible que los pipelines no sean adecuados.\n",
    "\n",
    " 1. Flujos de trabajo personalizados: Si requieres de hacer transformaciones que no se ajusten a la estructura secuencial de un pipeline de scikit-learn, es posible que debas manejar los pasos manualmente.\n",
    "\n",
    " 1. Modelos fuera de scikit-learn: Si estás utilizando modelos o herramientas de aprendizaje automático de otras bibliotecas que no siguen la API de scikit-learn, es posible que no puedas usar un pipeline directamente.\n",
    "\n",
    " 1. Si estás lidiando con enormes cantidades de datos: puede que a veces sea mejor llevar a cabo transformaciones de datos en otros lenguajes, como SQL para ahorrarnos tiempo.\n",
    "\n",
    "En resumen, los pipelines de scikit-learn son una herramienta poderosa para muchos flujos de trabajo de aprendizaje automático, pero pueden no ser adecuados para todas las situaciones. Considera las necesidades específicas y las limitaciones de tu proyecto antes de decidir si un pipeline es la mejor opción.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "notion_metadata": {
   "archived": false,
   "cover": null,
   "created_by": {
    "id": "84951847-6e2b-487e-acba-6838f60f1102",
    "object": "user"
   },
   "created_time": "2023-04-13T21:06:00.000Z",
   "icon": null,
   "id": "bc741fd9-f5d2-41dc-b6f6-375134986410",
   "last_edited_by": {
    "id": "84951847-6e2b-487e-acba-6838f60f1102",
    "object": "user"
   },
   "last_edited_time": "2023-04-21T17:09:00.000Z",
   "object": "page",
   "parent": {
    "database_id": "97ecfa4e-50d1-4827-8791-2199c709d680",
    "type": "database_id"
   },
   "properties": {
    "Assign": {
     "id": "%5DtZZ",
     "people": [],
     "type": "people"
    },
    "Code name": {
     "id": "PT%5CP",
     "rich_text": [],
     "type": "rich_text"
    },
    "Name": {
     "id": "title",
     "title": [
      {
       "annotations": {
        "bold": false,
        "code": false,
        "color": "default",
        "italic": false,
        "strikethrough": false,
        "underline": false
       },
       "href": null,
       "plain_text": "6.1.2 Pipelines (Parte 2)",
       "text": {
        "content": "6.1.2 Pipelines (Parte 2)",
        "link": null
       },
       "type": "text"
      }
     ],
     "type": "title"
    },
    "Order": {
     "id": "k_Cb",
     "number": 7.2,
     "type": "number"
    },
    "Real order": {
     "id": "%7Dn_k",
     "rich_text": [
      {
       "annotations": {
        "bold": false,
        "code": false,
        "color": "default",
        "italic": false,
        "strikethrough": false,
        "underline": false
       },
       "href": null,
       "plain_text": "30 - Pipelines",
       "text": {
        "content": "30 - Pipelines",
        "link": null
       },
       "type": "text"
      }
     ],
     "type": "rich_text"
    },
    "Status": {
     "id": "s%7D%5Ea",
     "status": {
      "color": "green",
      "id": "b6060d48-45e7-42e9-a8a0-8bd803231f7f",
      "name": "Done"
     },
     "type": "status"
    }
   },
   "url": "https://www.notion.so/6-1-2-Pipelines-Parte-2-bc741fd9f5d241dcb6f6375134986410"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
