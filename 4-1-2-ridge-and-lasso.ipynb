{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b5d874",
   "metadata": {},
   "source": [
    "# Otro tipo de regresiones\n",
    "\n",
    "Existen otros dos tipos de regresiones en scikit-learn: Lasso y Ridge. Estas se caracterizan por poner restricciones a la magnitud de los coeficientes de la regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f695c90",
   "metadata": {},
   "source": [
    "## Ridge\n",
    "\n",
    "La primera de las que les quiero hablar es conocida como Ridge.\n",
    "\n",
    "Esta regresión, a diferencia de la regresión lineal tradicional, se penaliza el la magnitud de los coeficientes aprendidos. Esto a su vez reduce la varianza de los coeficientes estimados y mejora la estabilidad del modelo cuando existe co-linealidad, es decir, la correlación entre nuestras variables predictoras.\n",
    "\n",
    "La penalización que utiliza Ridge es conocida como L2, te dejo más detalles sobre esta penalización en los recursos de la sesión para que conozcas más.\n",
    "\n",
    "Antes de comenzar, vamos a generar algunos datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23b5a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=10, bias=2.0, noise=5.0, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f841ad8b",
   "metadata": {},
   "source": [
    "Para usar ridge, hay que importarla de <code>linear_model</code>, y llamar al constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b8cc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f822c39",
   "metadata": {},
   "source": [
    "Y como todo otro estimador en scikit-learn, tiene los métodos <code>fit</code> y <code>predict</code> para interactuar con ella:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16382d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.fit(X_train, y_train)\n",
    "y_pred = ridge.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036f39ec",
   "metadata": {},
   "source": [
    "### Argumentos\n",
    "\n",
    "La clase comparte un par de argumentos con <code>LinearRegression</code>, estos son <code>fit_intercept</code> y <code>normalize</code>. Pero además incluye algunos específicos:\n",
    "\n",
    " - <code>alpha</code>: Es un parámetro de tipo flotante que especifica el nivel de regularización en el modelo. Un valor más alto de alpha da como resultado coeficientes más pequeños y, por lo tanto, un modelo más simplificado. El valor predeterminado es 1.0 – este es un hiperparámetro que es recomendable tunear.\n",
    "\n",
    " - <code>solver</code>: Es una cadena que indica el solucionador utilizado en el problema de optimización subyacente. Los valores posibles son \"auto\", \"svd\", \"cholesky\", \"lsqr\" y \"sparse_cg\". El valor predeterminado es \"auto\", y en general funciona bien.\n",
    "\n",
    " - <code>max_iter</code>: Es un entero que especifica el número máximo de iteraciones permitidas en el solucionador – algunos solucionadores funcionan de manera iterativa. El valor predeterminado es None, lo que significa que se utiliza un valor razonable en función del tamaño del conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e23bd42",
   "metadata": {},
   "source": [
    "## Lasso\n",
    "\n",
    "Esta regresión, a diferencia de la regresión lineal tradicional, se penaliza el la magnitud de los coeficientes aprendidos – parecido a la regresión Ridge.\n",
    "\n",
    "La penalización que utiliza Lasso es conocida como L1, te dejo más detalles sobre esta penalización en los recursos de la sesión para que conozcas más. Pero algo a notar es que Lasso puede forzar a que algunos coeficientes se vuelvan cero, excluyendo así algunas de las variables de entrada de los cálculos del modelo, reduciendo así la complejidad de este.\n",
    "\n",
    "Las variables \"castigadas\" son aquellas que el modelo considere irrelevantes o con alta colinearidad.\n",
    "\n",
    "El algoritmo de Lasso funciona de forma iterativa por definición, esa es otra diferencia con la regresión lineal tradicional que tiene una solución analítica cerrada.\n",
    "\n",
    "Ridge también está disponible en el módulo <code>linear_model</code> de <code>sklearn</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9aa85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071b310f",
   "metadata": {},
   "source": [
    "Y desde luego, comparte la interfaz de otros estimadores en scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9fd943",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.fit(X_train, y_train)\n",
    "y_pred = ridge.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde4cf84",
   "metadata": {},
   "source": [
    "### Argumentos\n",
    "\n",
    "Al igual que la regresión Ridge, también tiene los argumentos <code>alpha</code> para controlar la fuerza con la que aplica la penalización, así como el parámetro <code>max_iter</code> que tiene mayor importancia aquí porque este sí es un algoritmo completamente iterativo.\n",
    "\n",
    "Además tiene los siguientes argumentos que te pueden ayudar en el entrenamiento:\n",
    "\n",
    " - <code>tol</code>: Es la tolerancia para la convergencia del algoritmo de optimización. Si la diferencia entre dos iteraciones consecutivas es menor que <code>tol</code>, se considera que el algoritmo ha convergido. El valor predeterminado es 1e-4.\n",
    "\n",
    " - <code>warm_start</code>: Este parámetro es booleano y especifica si se deben utilizar los coeficientes de la regresión anterior como punto de partida para la regresión actual. Si es <code>True</code>, se utiliza la solución anterior como punto de partida para la optimización, lo que puede acelerar el proceso de ajuste. El valor predeterminado es <code>False</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f399db1",
   "metadata": {},
   "source": [
    "## Atributos\n",
    "\n",
    "Ambas clases ofrecen los atributos de la regresión lineal que nos ayudan a entender un poco más sobre nuestros valores de entrada. Si recuerdas, en la sesión anterior sobre regresión lineal vimos cómo es que los atributos <code>coef_</code> e <code>intercept_</code> pueden ser usados para interpretar los resultados. Lasso y Ridge cuentan con ellos también. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b4f268",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953d7b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import  make_regression\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "\n",
    "# Generate a random regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, noise=10, random_state=42)\n",
    "\n",
    "# Fit Linear Regression\n",
    "lr = LinearRegression()\n",
    "ridge = Ridge()\n",
    "lasso = Lasso()\n",
    "\n",
    "# Fit regressions\n",
    "lr.fit(X, y)\n",
    "ridge.fit(X, y)\n",
    "lasso.fit(X, y)\n",
    "\n",
    "# Plot the coefficients\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "models = ['Linear Regression', 'Ridge Regression', 'Lasso Regression']\n",
    "coefficients = [lr.coef_, ridge.coef_, lasso.coef_]\n",
    "colors = ['blue', 'green', 'red']\n",
    "\n",
    "for i, (model, coef) in enumerate(zip(models, coefficients)):\n",
    "    ax.bar(np.arange(len(coef)) + i*0.25, coef, color=colors[i], width=0.25, label=model)\n",
    "\n",
    "ax.set_xticks(np.arange(len(coef)))\n",
    "ax.set_xticklabels(['Feature '+str(i) for i in range(len(coef))])\n",
    "ax.set_ylabel(\"Coeficiente\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c182254",
   "metadata": {},
   "source": [
    "## ¿Cuando usar cada una?\n",
    "\n",
    " - La regresión lineal es una buena opción cuando la relación entre las variables independientes y la variable dependiente es aproximadamente lineal, siempre es un buen método a considerar, aunque sea para establecer un baseline.\n",
    "\n",
    " - La regresión de ridge es una buena opción cuando hay muchas características y se espera que algunas de ellas tengan efectos pequeños o moderados en la variable dependiente. Ridge ayuda en la regularización al encoger los coeficientes de algunas características hacia cero, pero no a cero como Lasso.\n",
    "\n",
    " - La regresión de lasso es una buena opción cuando hay muchas características y se espera que algunas de ellas sean irrelevantes o redundantes. Lasso ayuda en la selección de características y la regularización al convertir algunos coeficientes de características en cero, efectivamente desapareciéndolas del modelo.\n",
    "\n",
    "Y pues ahí lo tienes, otros dos tipos de regresión muy parecidos a la regresión lineal pero que incluyen un nivel de penalización para los coeficientes que nos ayuda a reducir overfitting y la complejidad general del modelo.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "notion_metadata": {
   "archived": false,
   "cover": null,
   "created_by": {
    "id": "84951847-6e2b-487e-acba-6838f60f1102",
    "object": "user"
   },
   "created_time": "2023-03-14T20:30:00.000Z",
   "icon": null,
   "id": "9cdbe641-ca87-495d-b530-d5975154bbd1",
   "last_edited_by": {
    "id": "84951847-6e2b-487e-acba-6838f60f1102",
    "object": "user"
   },
   "last_edited_time": "2023-04-21T17:07:00.000Z",
   "object": "page",
   "parent": {
    "database_id": "97ecfa4e-50d1-4827-8791-2199c709d680",
    "type": "database_id"
   },
   "properties": {
    "Assign": {
     "id": "%5DtZZ",
     "people": [],
     "type": "people"
    },
    "Code name": {
     "id": "PT%5CP",
     "rich_text": [],
     "type": "rich_text"
    },
    "Name": {
     "id": "title",
     "title": [
      {
       "annotations": {
        "bold": false,
        "code": false,
        "color": "default",
        "italic": false,
        "strikethrough": false,
        "underline": false
       },
       "href": null,
       "plain_text": "4.1.2 Ridge and Lasso",
       "text": {
        "content": "4.1.2 Ridge and Lasso",
        "link": null
       },
       "type": "text"
      }
     ],
     "type": "title"
    },
    "Order": {
     "id": "k_Cb",
     "number": 5.12,
     "type": "number"
    },
    "Real order": {
     "id": "%7Dn_k",
     "rich_text": [
      {
       "annotations": {
        "bold": false,
        "code": false,
        "color": "default",
        "italic": false,
        "strikethrough": false,
        "underline": false
       },
       "href": null,
       "plain_text": "21 - LassoAndRidge",
       "text": {
        "content": "21 - LassoAndRidge",
        "link": null
       },
       "type": "text"
      }
     ],
     "type": "rich_text"
    },
    "Status": {
     "id": "s%7D%5Ea",
     "status": {
      "color": "green",
      "id": "b6060d48-45e7-42e9-a8a0-8bd803231f7f",
      "name": "Done"
     },
     "type": "status"
    }
   },
   "url": "https://www.notion.so/4-1-2-Ridge-and-Lasso-9cdbe641ca87495db530d5975154bbd1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
